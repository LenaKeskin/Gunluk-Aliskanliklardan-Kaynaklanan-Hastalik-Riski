# -*- coding: utf-8 -*-
"""Untitled53.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RI-SRXgud47jCiKkabiLXdaPoiByp0eW

VERİ ANALİZİ

Gerekli Kütüphane ve Veri Yükleme
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Veriyi yükle
df = pd.read_csv('cleaned_health_data .csv')

"""Keşifsel Veri Analizi"""

# Genel görselleştirme ayarları
# plt.style.use('seaborn') # Removed this line
plt.rcParams['figure.figsize'] = (12, 6)

# 1. Hedef değişken dağılımı
plt.figure(figsize=(8,5))
sns.countplot(x='target', data=df)
plt.title('Hedef Değişken Dağılımı (0: Healthy, 1: Diseased)')
plt.show()

# 2. Sayısal değişkenlerin dağılımı
num_cols = ['age', 'bmi', 'blood_pressure', 'heart_rate', 'cholesterol']
df[num_cols].hist(bins=20, figsize=(15,10))
plt.suptitle('Sayısal Değişkenlerin Dağılımı')
plt.tight_layout()
plt.show()

# 3. Kategorik değişken analizi
cat_cols = ['gender', 'smoking_level', 'diet_type', 'exercise_type']
for col in cat_cols:
    plt.figure(figsize=(10,5))
    sns.countplot(x=col, hue='target', data=df)
    plt.title(f'{col} - Sağlık Durumuna Göre Dağılım')
    plt.xticks(rotation=45)
    plt.show()

"""Korelasyon Analizi"""

# Korelasyon analizi
plt.figure(figsize=(15,10))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Değişkenler Arası Korelasyon Matrisi')
plt.show()

"""BMI(Vücut Kitle Endeksi) ile Yaş İlişkisi"""

# BMI ve yaş ilişkisi
plt.figure(figsize=(10,6))
sns.scatterplot(x='age', y='bmi', hue='target', data=df, alpha=0.6)
plt.title('Yaş ve BMI İlişkisi')
plt.show()

# Sigara içme durumuna göre sağlık durumu
plt.figure(figsize=(8, 5))
sns.countplot(x='smoking_level', hue='target', data=df)
plt.title('Sigara İçme Durumuna Göre Sağlık Durumu')
plt.show()

"""İstatistiksel Analizler"""

from scipy.stats import ttest_ind, chi2_contingency

# 1. Healthy vs Diseased gruplarında sayısal değişkenlerin karşılaştırılması
print("Sayısal Değişkenlerin Karşılaştırılması (Healthy vs Diseased):")
for col in num_cols:
    healthy = df[df['target']==0][col]
    diseased = df[df['target']==1][col]
    t_stat, p_val = ttest_ind(healthy, diseased)
    print(f"{col}: t-stat={t_stat:.2f}, p-value={p_val:.4f}")

# 2. Kategorik değişkenlerin ki-kare testi
print("\nKategorik Değişkenlerin Ki-kare Test Sonuçları:")
for col in cat_cols:
    contingency_table = pd.crosstab(df[col], df['target'])
    chi2, p, dof, expected = chi2_contingency(contingency_table)
    print(f"{col}: chi2={chi2:.2f}, p-value={p:.4f}")

"""Boxplot ile Dağılım Karşılaştırması"""

# Örneğin egzersiz süresi
sns.boxplot(x='target', y='physical_activity', data=df)
plt.title("Fiziksel Aktivite Dağılımı (Hastalığa Göre)")
plt.show()

"""Z-score ile Aykırı (Outlier) Değer Tespiti"""

from scipy.stats import zscore

z_scores = zscore(df.select_dtypes(include=['float64', 'int64']))
outliers = (abs(z_scores) > 3).sum(axis=1)
df['outlier_count'] = outliers

# En çok outlier barındıran ilk 5 gözlem
print(df.sort_values('outlier_count', ascending=False).head())

"""Chi-Square Testi (Kategorik Değişkenler Arası İlişki)"""

from scipy.stats import chi2_contingency

contingency_table = pd.crosstab(df['smoking_level'], df['target'])
chi2, p, dof, expected = chi2_contingency(contingency_table)

print(f"Chi-Square Değeri: {chi2}")
print(f"P-Value: {p}")

"""Eğer p-value < 0.05 ise, “sigara kullanımı ile hastalık arasında istatistiksel olarak anlamlı bir ilişki var” denebilir.

Gruplandırılmış İstatistikler
"""

df.groupby('smoking_level')['target'].describe()

"""Yaş Gruplarına Göre Analiz"""

import pandas as pd
import matplotlib.pyplot as plt

# Örnek veri oluşturma
data = {
    'yas': [15, 25, 32, 47, 55, 70, 22, 38, 60, 28],
    'BMI': [18.5, 22.3, 25.1, 27.8, 24.9, 26.3, 21.7, 29.1, 23.8, 26.5],
    'sigara_iciyor': [0, 1, 1, 0, 1, 0, 0, 1, 0, 1]
}
df = pd.DataFrame(data)

# Yaş grupları oluşturma
bins = [0, 18, 35, 50, 65, 100]
labels = ['0-18', '19-35', '36-50', '51-65', '65+']
df['yas_grubu'] = pd.cut(df['yas'], bins=bins, labels=labels)

# Yaş gruplarına göre ortalama BMI
plt.figure(figsize=(10, 6))
df.groupby('yas_grubu')['BMI'].mean().plot(kind='bar', color='skyblue')
plt.title('Yaş Gruplarına Göre Ortalama BMI')
plt.xlabel('Yaş Grubu')
plt.ylabel('Ortalama BMI')
plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

"""Çoklu Değişken Analizi – Pairlot"""

import seaborn as sns

# Sadece sayısal sütunları filtrele
num_cols = df.select_dtypes(include='number').columns

# Pairplot çiz
sns.pairplot(df[num_cols])
plt.suptitle("Sayısal Değişkenler Arası İlişki", y=1.02)
plt.show()

"""Cinsiyete Göre Sağlık Profili Karşılaştırması"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Veri yükleme
df = pd.read_csv('cleaned_health_data .csv')

# Cinsiyete göre gruplama ve istatistikler
gender_stats = df.groupby('gender').agg({
    'bmi': 'mean',
    'blood_pressure': 'median',
    'heart_rate': 'mean',
    'stress_level': 'mean'
}).reset_index()

# Görselleştirme
plt.figure(figsize=(12, 6))
sns.barplot(x='gender', y='bmi', data=df, errorbar=None)
plt.title('Cinsiyete Göre Ortalama BMI Dağılımı')
plt.show()

# BMI kategorileri oluşturup analiz
df['bmi_category'] = pd.cut(df['bmi'],
                           bins=[0, 18.5, 25, 30, float('inf')],
                           labels=['underweight', 'normal', 'overweight', 'obese'])

"""Uyku Kalitesi ile Ruh Sağlığı Arasındaki İlişki"""

# Korelasyon analizi
correlation = df[['sleep_quality', 'mental_health_score']].corr()

# Scatter plot
sns.lmplot(x='sleep_quality', y='mental_health_score', data=df, hue='gender')
plt.title('Uyku Kalitesi ve Ruh Sağlığı İlişkisi')
plt.show()

"""Stres – Ruh Sağlığı – Fiziksel Aktivite Üçlü İlişkisi"""

sns.scatterplot(x='stress_level', y='mental_health_score',
                size='physical_activity', hue='gender', data=df)
plt.title('Üçlü İlişki Analizi')
plt.show()

# 3D plot (isteğe bağlı)
from mpl_toolkits.mplot3d import Axes3D
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.scatter(df['stress_level'], df['mental_health_score'], df['physical_activity'])
ax.set_xlabel('Stres')
ax.set_ylabel('Ruh Sağlığı')
ax.set_zlabel('Fiziksel Aktivite')
plt.show()

"""Günlük Adım Sayısına Göre Hastalık Riski Dağılımı"""

# Adım sayısına göre kategorize etme
df['step_category'] = pd.cut(df['daily_steps'],
                            bins=[0, 5000, 10000, float('inf')],
                            labels=['Düşük', 'Orta', 'Yüksek'])

# Risk dağılımı
sns.countplot(x='step_category', hue='target', data=df)
plt.title('Adım Sayısı ve Hastalık Riski')
plt.show()

"""Aile Geçmişi ve Çevresel Faktörlerin Etkisi"""

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))

# Aile geçmişi ile çevresel faktörlerin (sigara, alkol, stres) ilişkisi
sns.boxplot(x='family_history', y='stress_level',
            hue='smoking_level', data=df)
plt.title('Aile Geçmişi ve Stres Seviyesi (Sigara Kullanımına Göre)')
plt.xlabel('Ailede Hastalık Geçmişi (1: Var, 0: Yok)')
plt.ylabel('Stres Seviyesi')
plt.legend(title='Sigara Seviyesi')
plt.show()

"""Çevresel Risk Skoru Oluşturma"""

# Çevresel risk skoru hesapla (örnek formül)
df['env_risk_score'] = (
    df['smoking_level'] * 0.3 +
    df['alcohol_consumption'] * 0.2 +
    df['stress_level'] * 0.2 +
    (1 - df['sleep_quality']/10) * 0.2 +
    df['screen_time']/10 * 0.1
)

# BMI ile çevresel risk ilişkisi
sns.lmplot(x='env_risk_score', y='bmi',
           hue='bmi_category', data=df, height=6)
plt.title('Çevresel Risk Skoru ve BMI İlişkisi')
plt.xlabel('Çevresel Risk Skoru (0-1 aralığında)')
plt.ylabel('BMI Değeri')
plt.show()

"""Genetik Yerine Biyometrik İşaretçiler"""

# Biyometrik belirteçlerin dağılımı (kolesterol, glukoz, tansiyon)
bio_vars = ['cholesterol', 'glucose', 'blood_pressure']

plt.figure(figsize=(15, 5))
for i, var in enumerate(bio_vars, 1):
    plt.subplot(1, 3, i)
    sns.violinplot(x='family_history', y=var, data=df)
    plt.title(f'Aile Geçmişi - {var.capitalize()} İlişkisi')
plt.tight_layout()
plt.show()

"""Destekleyici Analiz (Beslenme ve Çevresel Etki)"""

# Beslenme faktörlerinin çevresel riskle ilişkisi
nutrition_vars = ['diet_type', 'calorie_intake', 'sugar_intake']

g = sns.PairGrid(df, x_vars=nutrition_vars,
                y_vars=['env_risk_score'], height=4)
g.map(sns.regplot)
g.fig.suptitle('Beslenme Faktörleri ve Çevresel Risk İlişkisi', y=1.05)
plt.show()

"""Tüm Faktörlerin Kombine Etkisi"""

from sklearn.preprocessing import StandardScaler

# Önemli faktörleri seçme
factors = ['env_risk_score', 'family_history', 'bmi', 'age']
X = df[factors].dropna()

# Standardizasyon
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Korelasyon matrisi
plt.figure(figsize=(10, 8))
sns.heatmap(pd.DataFrame(X_scaled, columns=factors).corr(),
            annot=True, cmap='coolwarm', center=0)
plt.title('Faktörler Arası Korelasyon Matrisi')
plt.show()

"""Sağlık Sigortası ve Hedef Sınıf Dağılımı"""

pd.crosstab(df['insurance'], df['target']).plot(kind='bar', stacked=True)
plt.title('Sigorta Durumuna Göre Hastalık Riski')
plt.ylabel('Kişi Sayısı')
plt.show()

"""Beslenme Faktörleri ile Risk İlişkisi"""

nutrition_cols = ['calorie_intake', 'sugar_intake', 'caffeine_intake']
sns.pairplot(df[nutrition_cols + ['target']], hue='target')
plt.suptitle('Beslenme Faktörleri ve Risk İlişkisi', y=1.02)
plt.show()

"""Metinsel Veri İçermeyen Bir Veri Setine NLP Teknikleri Uygulanması

Sayısal/Kategorik Sütunları Metne Dönüştürerek NLP
"""

# Örnek: BMI kategorileri oluşturup analiz
df['bmi_category'] = pd.cut(df['bmi'],
                           bins=[0, 18.5, 25, 30, float('inf')],
                           labels=['underweight', 'normal', 'overweight', 'obese'])

# BMI kategorilerinin dağılımı
from wordcloud import WordCloud
text_data = " ".join(df['bmi_category'].dropna().astype(str))
wordcloud = WordCloud(width=800, height=400).generate(text_data)
plt.imshow(wordcloud)
plt.axis("off")
plt.title("BMI Kategorileri Dağılımı")
plt.show()

"""Sayısal Verilerden Metinsel Özet Çıkarma"""

from sklearn.preprocessing import KBinsDiscretizer

# Sayısal sütunları kategorik hale getirme
discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='quantile')
df['stress_level_category'] = discretizer.fit_transform(df[['stress_level']]).astype(int)

# Kategorik değerleri metne çevirme
stress_labels = {0: 'low stress', 1: 'medium stress', 2: 'high stress'}
df['stress_label'] = df['stress_level_category'].map(stress_labels)

# Metinsel analiz
text = " ".join(df['stress_label'])
print(f"Stres durumu özeti: {text[:200]}...")  # İlk 200 karakter

"""Yapay Metin Üretme ve Analiz"""

# Mevcut verilerden sentetik metin üretme
def generate_health_summary(row):
    return (f"{row['gender']}, {row['age']} yaşında. "
            f"BMI: {row['bmi']:.1f}. "
            f"Stres seviyesi: {row['stress_level']}/10. "
            f"Uyku kalitesi: {row['sleep_quality']}/10.")

df['health_summary'] = df.apply(generate_health_summary, axis=1)

# Örnek çıktılar
print(df['health_summary'].head(3))

# Basit kelime analizi
from collections import Counter
words = " ".join(df['health_summary']).lower().split()
common_words = Counter(words).most_common(20)
print("\nEn sık kullanılan 20 kelime:", common_words)

"""Veri Seti Özeti Oluşturma (NLP Formatında)"""

# Tüm veri seti için metinsel özet
summary = f"""
Veri seti özeti:
- Toplam {len(df)} hasta kaydı
- Ortalama yaş: {df['age'].mean():.1f}
- Cinsiyet dağılımı: {df['gender'].value_counts().to_dict()}
- En yaygın 3 BMI kategorisi: {df['bmi_category'].value_counts().nlargest(3).to_dict()}
"""

print(summary)

# Özeti dosyaya kaydetme
with open("dataset_summary.txt", "w") as f:
    f.write(summary)

"""Tüm Analizleri Otomatikleştiren Fonksiyon"""

def auto_analyze(df):
    reports = []

    # 1. Gender Analysis
    gender_report = df.groupby('gender').mean(numeric_only=True)
    reports.append(("Cinsiyet Analizi", gender_report))

    # 2. Correlation Matrix
    # Select only numeric columns before calculating correlation
    numeric_df = df.select_dtypes(include=['number'])
    corr_matrix = numeric_df.corr()
    reports.append(("Korelasyon Matrisi", corr_matrix))

    # 3. Target Distribution
    target_dist = df['target'].value_counts(normalize=True)
    reports.append(("Hedef Dağılımı", target_dist))

    return reports

# Raporları görüntüleme
for title, report in auto_analyze(df):
    print(f"\n{title}\n{'='*30}")
    print(report)

"""Spark Session Başlatma"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, mean, stddev
import pandas as pd

# Spark session oluşturma (4 CPU çekirdeği kullanarak)
spark = SparkSession.builder \
    .appName("HealthDataAnalysis") \
    .master("local[4]") \
    .config("spark.driver.memory", "4g") \
    .getOrCreate()

# Load the data into a Pandas DataFrame first
df = pd.read_csv('cleaned_health_data .csv')

# Create the 'step_category' column in the Pandas DataFrame
df['step_category'] = pd.cut(df['daily_steps'],
                            bins=[0, 5000, 10000, float('inf')],
                            labels=['Düşük', 'Orta', 'Yüksek'])

# Create the 'bmi_category' column in the Pandas DataFrame
df['bmi_category'] = pd.cut(df['bmi'],
                           bins=[0, 18.5, 25, 30, float('inf')],
                           labels=['underweight', 'normal', 'overweight', 'obese'])


# Convert Pandas DataFrame to Spark DataFrame
spark_df = spark.createDataFrame(df)
# Aşağıdaki satır, kod geliştirme ve test süreçlerini hızlandırmak için
# Tüm veri yerine %10'luk bir örnekle çalışarak kodu hızlıca test et
# df_sample = df.sample(frac=0.1)

# Veya sadece ilk 1000 satırı al
# df_sample = df.limit(1000)

# Geliştirmenizi df_sample üzerinde yapın

"""Veri Keşfi (Exploratory Data Analysis)"""

# Temel istatistikler
numeric_cols = ['age', 'bmi', 'stress_level', 'heart_rate']
spark_df.select(numeric_cols).describe().show()

# Korelasyon analizi
from pyspark.ml.stat import Correlation
from pyspark.ml.feature import VectorAssembler

vector_col = "features"
assembler = VectorAssembler(inputCols=numeric_cols, outputCol=vector_col)
df_vector = assembler.transform(spark_df).select(vector_col)

corr_matrix = Correlation.corr(df_vector, vector_col).collect()[0][0]
print("Korelasyon Matrisi:\n", corr_matrix.toArray())

"""Veri Temizleme ve Ön İşleme"""

# Eksik değerleri median ile doldurma
from pyspark.sql.functions import lit

for col_name in numeric_cols:
    median_val = spark_df.approxQuantile(col_name, [0.5], 0.01)[0]
    spark_df = spark_df.fillna(median_val, subset=[col_name])

# Aykırı değer kontrolü (IQR yöntemi)
for col_name in numeric_cols:
    stats = spark_df.select(
        mean(col(col_name)).alias('mean'),
        stddev(col(col_name)).alias('std')
    ).collect()[0]

    spark_df = spark_df.withColumn(
        f"{col_name}_outlier",
        when(
            (col(col_name) > stats['mean'] + 3*stats['std']) |
            (col(col_name) < stats['mean'] - 3*stats['std']),
            lit(1)
        ).otherwise(lit(0))
    )

# Kategorik değişkenleri işleme
from pyspark.ml.feature import StringIndexer

categorical_cols = ['gender', 'bmi_category', 'step_category']
indexers = [StringIndexer(inputCol=col, outputCol=col+"_index") for col in categorical_cols]

for indexer in indexers:
    spark_df = indexer.fit(spark_df).transform(spark_df)

"""Feature Engineering"""

from pyspark.ml.feature import VectorAssembler, StandardScaler

# Özellik vektörü oluşturma
feature_cols = numeric_cols + [col+"_index" for col in categorical_cols]
assembler = VectorAssembler(inputCols=feature_cols, outputCol="raw_features")

# Ölçeklendirme
scaler = StandardScaler(inputCol="raw_features", outputCol="scaled_features")

# Pipeline oluşturma
from pyspark.ml import Pipeline
pipeline = Pipeline(stages=[assembler, scaler])
processed_df = pipeline.fit(spark_df).transform(spark_df)

"""Makine Öğrenmesi (PySpark ML)"""

from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

# Train-test ayırma
train_df, test_df = processed_df.randomSplit([0.7, 0.3], seed=42)

# Model eğitimi
rf = RandomForestClassifier(
    featuresCol="scaled_features",
    labelCol="target",
    numTrees=100,
    maxDepth=5
)

model = rf.fit(train_df)

# Tahminler
predictions = model.transform(test_df)

# Metrikler
evaluator = MulticlassClassificationEvaluator(labelCol="target")
print(f"Accuracy: {evaluator.evaluate(predictions, {evaluator.metricName: 'accuracy'})}")
print(f"F1 Score: {evaluator.evaluate(predictions, {evaluator.metricName: 'f1'})}")

"""Görselleştirme (Spark -> Pandas)"""

# Spark DataFrame'den örneklem alarak görselleştirme
sample_pd = predictions.sample(0.1).toPandas()

import seaborn as sns
import matplotlib.pyplot as plt

# BMI dağılımı
plt.figure(figsize=(10,6))
sns.boxplot(x='target', y='bmi', data=sample_pd)
plt.title("BMI vs Hastalık Riski")
plt.show()

# Confusion matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(sample_pd['target'], sample_pd['prediction'])
sns.heatmap(cm, annot=True, fmt='d')
plt.title("Confusion Matrix")
plt.show()

"""Optimizasyon (Cross-Validation)"""

from pyspark.ml.tuning import ParamGridBuilder, CrossValidator

param_grid = ParamGridBuilder() \
    .addGrid(rf.numTrees, [50, 100]) \
    .addGrid(rf.maxDepth, [3, 5]) \
    .build()

cv = CrossValidator(
    estimator=rf,
    estimatorParamMaps=param_grid,
    evaluator=evaluator,
    numFolds=3
)

cv_model = cv.fit(train_df)
print("En iyi model F1 skoru:", cv_model.avgMetrics[0])

"""Sonuçları Kaydetme"""

# Modeli kaydetme
model.write().overwrite().save("spark_random_forest_model")

# Predictions'ı CSV olarak yazma
predictions.select("target", "prediction").write.mode("overwrite").csv("predictions_output")

"""Spark UI Erişimi"""

# Spark UI'ı görüntülemek için (localhost:4040)
input("Analiz tamamlandı. Spark UI için http://localhost:4040 adresini ziyaret edin...")

"""MAKİNE ÖĞRENMESİ

Veri Hazırlama
"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, f1_score
import pandas as pd

# Recreate the columns that were lost after reloading the DataFrame
# Sayısal sütunları kategorik hale getirme
from sklearn.preprocessing import KBinsDiscretizer
discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='quantile')
df['stress_level_category'] = discretizer.fit_transform(df[['stress_level']]).astype(int)

# Kategorik değerleri metne çevirme
stress_labels = {0: 'low stress', 1: 'medium stress', 2: 'high stress'}
df['stress_label'] = df['stress_level_category'].map(stress_labels)

# Mevcut verilerden sentetik metin üretme
def generate_health_summary(row):
    return (f"{row['gender']}, {row['age']} yaşında. "
            f"BMI: {row['bmi']:.1f}. "
            f"Stres seviyesi: {row['stress_level']}/10. "
            f"Uyku kalitesi: {row['sleep_quality']}/10.")

df['health_summary'] = df.apply(generate_health_summary, axis=1)


# Özellikler ve hedef değişken
X = df.drop(['target', 'health_summary', 'stress_label'], axis=1)  # Metin sütunlarını çıkar
y = df['target']

# Kategorik değişkenleri işleme
X = pd.get_dummies(X, columns=['bmi_category', 'step_category'])

# Eksik verileri doldurma
X.fillna(X.mean(), inplace=True)

# Veriyi ölçeklendirme
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Eğitim-test ayırma
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

"""K-Means Kümeleme"""

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

silhouette_scores = []
for k in range(2, 6):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_scaled)
    score = silhouette_score(X_scaled, kmeans.labels_)
    silhouette_scores.append(score)
    print(f"K={k}, Silhouette Score: {score:.3f}")

# En iyi k değeri ile modelleme
best_k = silhouette_scores.index(max(silhouette_scores)) + 2
kmeans = KMeans(n_clusters=best_k, random_state=42)
clusters = kmeans.fit_predict(X_scaled)

# Kümeleri veri setine ekleme
df['cluster'] = clusters

"""K-Means için Görselleştirme"""

from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

plt.scatter(X_pca[:,0], X_pca[:,1], c=clusters, cmap='viridis')
plt.title('K-Means Kümeleme (2D PCA)')
plt.show()

"""XGBoost"""

from xgboost import XGBClassifier

xgb = XGBClassifier(random_state=42, eval_metric='logloss')
xgb.fit(X_train, y_train)
y_pred = xgb.predict(X_test)

print("XGBoost Performans:")
print(classification_report(y_test, y_pred))
print(f"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.4f}")

"""Random Forest"""

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)

print("\nRandom Forest Performans:")
print(classification_report(y_test, y_pred))
print(f"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.4f}")

"""Karar Ağaçları"""

from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)
y_pred = dt.predict(X_test)

print("\nKarar Ağacı Performans:")
print(classification_report(y_test, y_pred))
print(f"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.4f}")

"""Naive Bayes"""

from sklearn.naive_bayes import GaussianNB

nb = GaussianNB()
nb.fit(X_train, y_train)
y_pred = nb.predict(X_test)

print("\nNaive Bayes Performans:")
print(classification_report(y_test, y_pred))
print(f"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.4f}")

"""SVR (Support Vector Regression)"""

from sklearn.svm import SVC

svr = SVC(kernel='rbf', random_state=42)
svr.fit(X_train, y_train)
y_pred = svr.predict(X_test)

print("\nSVR Performans:")
print(classification_report(y_test, y_pred))
print(f"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.4f}")

"""Model Karşılaştırma"""

import pandas as pd
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.metrics import f1_score
import time # Süreyi ölçmek için time kütüphanesini ekleyelim

# Modelleri tanımla (Şimdilik en yavaş olan SVM'i yorum satırı yapabiliriz)
models = {
    'XGBoost': XGBClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Naive Bayes': GaussianNB(),
    #'SVM': SVC(kernel='rbf', random_state=42) # <-- Test için geçici olarak devre dışı bırak
}

results = []
print("Model eğitimi ve değerlendirme süreci başladı...")
print("-" * 50)

# Döngü içine teşhis amaçlı print ve süre ölçümü ekleyelim
for name, model in models.items():
    print(f"[{time.strftime('%H:%M:%S')}] Model eğitiliyor: {name}...")

    start_time = time.time()

    # 1. Eğitim
    model.fit(X_train, y_train)

    # 2. Tahmin
    y_pred = model.predict(X_test)

    # 3. Skoru Hesaplama
    f1 = f1_score(y_test, y_pred, average='weighted')

    end_time = time.time()
    duration = end_time - start_time

    print(f"-> {name} tamamlandı. Süre: {duration:.2f} saniye. F1 Skoru: {f1:.4f}")

    results.append({'Model': name, 'F1 Score': f1, 'Eğitim Süresi (s)': duration})

print("-" * 50)
print("Tüm modellerin değerlendirmesi tamamlandı.")

# Sonuçları DataFrame'e çevir ve göster
results_df = pd.DataFrame(results).sort_values('F1 Score', ascending=False)

print("\nModel Karşılaştırması:")
print(results_df)

"""Hiperparametre Optimizasyonu"""

from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5, 7]
}
grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)
grid.fit(X_train, y_train)
print("En iyi parametreler:", grid.best_params_)

"""Özellik Önem Sıralaması"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier

# 1. Modeli oluşturun (Henüz eğitilmedi)
# Projenin tutarlılığı için önceki karşılaştırmadakiyle aynı random_state'i kullanalım.
rf_model = RandomForestClassifier(random_state=42)

# 2. Modeli EĞİTİN (En önemli adım! Bu satır olmadan .feature_importances_ çalışmaz)
# X_train ve y_train'in daha önceki adımlarda tanımlandığını varsayıyoruz.
rf_model.fit(X_train, y_train)

# 3. Model EĞİTİLDİKTEN SONRA özellik önemlerine erişin
# Artık rf_model nesnesinin içinde 'feature_importances_' özniteliği mevcut.
importances = rf_model.feature_importances_

# 4. Sonuçları daha anlaşılır bir şekilde görselleştirin
# X_train'in sütun isimlerini kullanarak bir Pandas Serisi oluşturun.
feat_importances = pd.Series(importances, index=X.columns)

print("En Önemli 10 Özellik (Random Forest):")
print(feat_importances.nlargest(10))

# En önemli 10 özelliği alıp yatay çubuk grafiği olarak çizdirin
plt.figure(figsize=(10, 6)) # Grafik boyutunu ayarlayalım
feat_importances.nlargest(10).plot(kind='barh', color='skyblue')
plt.title('Random Forest - En Önemli 10 Özellik', size=15)
plt.xlabel('Önem Skoru', size=12)
plt.ylabel('Özellikler', size=12)
plt.gca().invert_yaxis() # En önemli özelliği en üste almak için
plt.show()

"""DERİN ÖĞRNENME

Kütüphaneleri Yükleme
"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np

"""Veri Yükleme ve Ön İşleme"""

# Load the data
df = pd.read_csv("cleaned_health_data .csv")

# Recreate the columns that were lost after reloading the DataFrame
# Sayısal sütunları kategorik hale getirme
from sklearn.preprocessing import KBinsDiscretizer

# Identify numeric columns to apply discretizer to, excluding those not suitable
numeric_cols_for_discretizer = ['stress_level'] # Add other numeric columns here if needed

for col_name in numeric_cols_for_discretizer:
    if col_name in df.columns and df[col_name].dtype in ['float64', 'int64']:
        discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='quantile')
        # Handle potential NaN values before applying discretizer
        # Store the discretized values in a temporary variable
        discretized_values = discretizer.fit_transform(df[[col_name]].dropna()).astype(int)
        # Create a new Series with the same index as the original column, filling NaN values with a placeholder
        discretized_series = pd.Series(discretized_values.flatten(), index=df[[col_name]].dropna().index)
        # Assign the discretized values back to the original DataFrame, aligning by index
        df[f'{col_name}_category'] = discretized_series
        # Kategorik değerleri metne çevirme
        labels = {0: f'low {col_name}', 1: f'medium {col_name}', 2: f'high {col_name}'}
        df[f'{col_name}_label'] = df[f'{col_name}_category'].map(labels)
    else:
        print(f"Column '{col_name}' not found or is not numeric, skipping discretization.")


# Mevcut verilerden sentetik metin üretme
def generate_health_summary(row):
    # Safely access columns, returning None if any are missing or NaN
    try:
        return (f"{row['gender']}, {row['age']} yaşında. "
                f"BMI: {row['bmi']:.1f}. "
                f"Stres seviyesi: {row['stress_level']}/10. "
                f"Uyku kalitesi: {row['sleep_quality']}/10.")
    except KeyError:
        return None


# Apply only to rows where required columns are not NaN
required_cols_for_summary = ['stress_level', 'sleep_quality', 'gender', 'age', 'bmi']
df['health_summary'] = df.apply(lambda row: generate_health_summary(row) if pd.notna(row[required_cols_for_summary]).all() else None, axis=1)


# Özellikler ve hedef değişken
# Drop columns that are not suitable for direct use in the model or have been transformed
cols_to_drop = ['target', 'health_summary', 'stress_level_label', 'survey_code', 'outlier_count', 'cluster']
existing_cols_to_drop = [col for col in cols_to_drop if col in df.columns]

X = df.drop(existing_cols_to_drop, axis=1)
y = df['target']

# Categorical columns to encode
categorical_cols_to_encode = ['bmi_category', 'step_category', 'gender', 'smoking_level', 'diet_type', 'exercise_type', 'job_type', 'occupation', 'education_level']
existing_categorical_cols = [col for col in categorical_cols_to_encode if col in X.columns and X[col].dtype == 'object']

# One-hot encode categorical columns
X = pd.get_dummies(X, columns=existing_categorical_cols, dummy_na=False)

# Eksik verileri doldurma
# Select only numeric columns for mean calculation
numeric_cols_for_fillna = X.select_dtypes(include=np.number).columns
X[numeric_cols_for_fillna] = X[numeric_cols_for_fillna].fillna(X[numeric_cols_for_fillna].mean())

# Drop rows where the target is NaN after dropping other columns
X = X.loc[y.dropna().index]
y = y.dropna()

# Explicitly define the final list of feature columns to ensure consistency
# This step might need manual adjustment based on the expected final columns after one-hot encoding
# For now, we'll assume all current columns in X are features
feature_columns = X.columns

# Veriyi ölçeklendirme
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Eğitim-test ayırma
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Ensure X_train and X_test have the same columns as feature_columns
X_train = pd.DataFrame(X_train, columns=feature_columns)
X_test = pd.DataFrame(X_test, columns=feature_columns)

"""Model Mimarisi ve Eğitimi"""

from tensorflow.keras.layers import Input

# Model mimarisi
model = Sequential([
    Input(shape=(X_train.shape[1],)), # Use an explicit Input layer
    Dense(128, activation='relu'),
    BatchNormalization(),
    Dropout(0.3),
    Dense(64, activation='relu'),
    BatchNormalization(),
    Dropout(0.3),
    Dense(32, activation='relu'),
    BatchNormalization(),
    Dropout(0.3),
    Dense(1, activation='sigmoid') # Binary classification output
])

# Modeli derleme
model.compile(optimizer=Adam(learning_rate=0.001),
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Modeli eğitme
history = model.fit(X_train, y_train,
                    epochs=100,
                    batch_size=32,
                    validation_split=0.2, # Use a validation split
                    callbacks=[early_stopping],
                    verbose=1)

"""Model Değerlendirmesi"""

# Modeli test seti üzerinde değerlendirme
loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f"Test Seti Doğruluğu: {accuracy:.4f}")

# Eğitim geçmişini görselleştirme
import matplotlib.pyplot as plt # Import matplotlib.pyplot
plt.figure(figsize=(12, 6))

# Doğruluk grafiği
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Eğitim Doğruluğu')
plt.plot(history.history['val_accuracy'], label='Doğrulama Doğruluğu')
plt.title('Eğitim ve Doğrulama Doğruluğu')
plt.xlabel('Epoch')
plt.ylabel('Doğruluk')
plt.legend()

# Kayıp (Loss) grafiği
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Eğitim Kaybı')
plt.plot(history.history['val_loss'], label='Doğrulama Kaybı')
plt.title('Eğitim ve Doğrulama Kaybı')
plt.xlabel('Epoch')
plt.ylabel('Kayıp')
plt.legend()

plt.tight_layout()
plt.show()

"""Hiperparametre Optimizasyonu"""

# Keras Tuner ile otomatik tuning
!pip install keras-tuner
import keras_tuner as kt

def build_model(hp):
    model = Sequential()
    model.add(Dense(units=hp.Int('units1', min_value=32, max_value=256, step=32),
               activation='relu', input_shape=(X_train.shape[1],)))
    model.add(Dense(units=hp.Int('units2', min_value=32, max_value=128, step=32), activation='relu'))
    model.add(Dense(1, activation='sigmoid'))

    model.compile(
        optimizer=Adam(hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy']
    )
    return model

tuner = kt.RandomSearch(build_model, objective='val_accuracy', max_trials=5)
tuner.search(X_train, y_train, epochs=30, validation_split=0.2)

# En iyi modeli al
best_model = tuner.get_best_models()[0]

"""Çapraz Doğrulama (Cross-Validation)"""

from sklearn.model_selection import KFold

kfold = KFold(n_splits=5, shuffle=True)
for train_idx, val_idx in kfold.split(X_train):
    X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]
    y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]
    model.fit(X_train_fold, y_train_fold, validation_data=(X_val_fold, y_val_fold), epochs=10)

"""Öğrenme Oranı Zamanlaması"""

from tensorflow.keras.callbacks import LearningRateScheduler

def lr_scheduler(epoch, lr):
    if epoch < 10:
        return lr
    else:
        return lr * tf.math.exp(-0.1)

# Use X_train and y_train for training, and include the callback
model.fit(X_train, y_train, callbacks=[LearningRateScheduler(lr_scheduler)])

"""Karışıklık Matrisi (Confusion Matrix)"""

from sklearn.metrics import confusion_matrix
import seaborn as sns
import numpy as np

# Tahminleri al
y_pred = (model.predict(X_test) > 0.5).astype(int)  # Binary için
cm = confusion_matrix(y_test, y_pred)

# Heatmap olarak çizdir
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.title('Confusion Matrix')
plt.show()

"""ROC Eğrisi ve AUC Skoru"""

from sklearn.metrics import roc_curve, auc

# ROC hesapla
fpr, tpr, thresholds = roc_curve(y_test, model.predict(X_test))
roc_auc = auc(fpr, tpr)

# Çizdir
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc="lower right")
plt.show()

"""Sınıf Dağılımları (Data Imbalance Check)"""

import pandas as pd

# Sınıf dağılımını kontrol et
class_dist = pd.Series(y_train).value_counts()

# Barplot çiz
plt.figure(figsize=(6, 4))
sns.barplot(x=class_dist.index, y=class_dist.values, palette="viridis")
plt.title('Class Distribution in Training Data')
plt.xlabel('Class')
plt.ylabel('Count')
plt.show()